### abstract
尽管大语言模型（LLMs）在语言理解和交互式决策任务中展现出了令人印象深刻的性能，但其推理能力（如思维链提示）和行动能力（如行动计划生成）在很大程度上一直被作为独立的课题进行研究。在本文中，我们探索了利用大语言模型以**交替（interleaved）**的方式生成推理轨迹（reasoning traces）和任务特定行动的方法，从而实现两者之间更强大的协同效应：推理轨迹有助于模型归纳、追踪和更新行动计划，并处理异常情况；而行动则允许模型与外部来源（如知识库或环境）进行交互并获取额外信息。 我们将这种命名为 **ReAct** 的方法应用于多种语言和决策任务，并证明了其相较于最先进（SOTA）基准模型的有效性，同时提高了人类可解释性和可信度。具体而言，在问答（HotpotQA）和事实核查（Fever）任务中，ReAct 通过与简单的维基百科 API 交互，克服了思维链推理中普遍存在的**幻觉（hallucination）**和**错误传播（error propagation）**问题，并生成了比缺乏推理轨迹的基准模型更具可解释性、类似人类的任务解决轨迹。此外，在两个交互式决策基准测试（ALFWorld 和 WebShop）上，ReAct 在仅使用一两个上下文示例（in-context examples）进行提示的情况下，其绝对成功率就分别超过了模仿学习和强化学习方法 34% 和 10%。